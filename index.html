<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Qian Zheng, PH, ZJU, Zhejiang University"> 
<meta name="description" content="Qian Zheng's homepage">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Qian Zheng's homepage, Zhejiang University</title>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Georgia"> Qian Zheng </font>   <font face="STKaiTi" size=6 color="0000">  郑   乾</font></h1>
				</div>

				<h3><font face="Georgia" color=000 size=4> <b>Assistant Professor</b> </font></h3>
				<p><font face="Georgia"> 
					College of Computer Science and Technology, <br>
					Zhejiang University, Hangzhou, China. <br>
				
					Email: <a href="qianzheng@zju.edu.cn">qianzheng@zju.edu.cn</a> <br>
					Official Academic Profile: [<a href="https://person.zju.edu.cn/en/zq">En</a>][<a href="https://person.zju.edu.cn/zq"><font face="STKaiTi">中文</font></a>]<br>
				</font></p>

				<p> <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ztWK59MAAAAJ"><img src="./figs/google_scholariconv1.png" height="80px" style="margin-bottom:-3px"></a>

			  <a href="#News"><img src="./figs/news_iconv1.png" height="80px" style="margin-bottom:-3px"></a>
				
				<a href="#Publications"><img src="./figs/publication_iconv1.png" height="80px" style="margin-bottom:-3px"></a>

				<a href="#Activity"><img src="./figs/activity_iconv1.png" height="80px" style="margin-bottom:-3px"></a>

				<a href="#Teaching"><img src="./figs/teachingv1.png" height="80px" style="margin-bottom:-3px"></a>

				   
				</p>
			</td>

			<td>
				<img src="./figs/profile.jpg" border="0" width="360"><br>
			</td>

		</tr><tr>
	</tr></tbody>
</table>


</head>

<body>
	

<div id="about">
<p style="text-align:justify";><font face="Georgia">
	I'm an Assistant Professor (<font face="STKaiTi">百人计划研究员</font>) at the College of Computer Science and Technology, <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>, and work closely with Prof. <a href="https://person.zju.edu.cn/en/gpan">Gang Pan</a> and Prof. <a href="https://person.zju.edu.cn/en/0018196"> Huajin Tang </a>. I received my B.E. (2011) and Ph.D. (2017, supervisor: Prof. <a href="https://person.zju.edu.cn/en/gpan">Gang Pan</a>) degrees in computer science from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. From 2018 to 2022, I was a Research Fellow at the <a href="https://www.ntu.edu.sg/rose">ROSE Lab</a>, Nanyang Technological University and worked closely with Prof. <a href="https://camera.pku.edu.cn/">Boxin Shi</a>, Prof. <a href="https://personal.ntu.edu.sg/exdjiang/">Xudong Jiang</a>, and Prof. <a href="https://personal.ntu.edu.sg/eackot/">Alex Kot</a>.
</font></p>
<p><font face="Georgia">
	I have co-authored over 60 peer-reviewed papers, with over 10 papers in the IEEE journals and over 30 papers in top conferences of CVPR/ICCV/ECCV/NeurIPS/ICML/ICLR/AAAI. I was a recipient of the ACM Rising Star Award (Hangzhou Chapter, 2024) and currently serves as an Associate Editor for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989"> IEEE Transactions on Cognitive and Developmental Systems </a> and <a href="https://www.sciencedirect.com/journal/neurocomputing">Neurocomputing</a>. 
</font></p>
<p><font face="Georgia">
	My research interests include brain-inspired computing, spiking neural networks, and computer vision. 
</font></p>

<p>
	
  
	<font face="STKaiTi" color=#BD417F>  
		<b>常年招收: 博士后、博士、硕士、本科实习生，欢迎邮件联系。希望你：</b>
		<ul>
	  		<li><b>追求</b>：科学严谨的逻辑思维、领域前沿的知识体系、良好的写作表达沟通等习惯</li>
	  		<li><b>认可</b>：文章质量远大于数量、科学研究是关于探索和创造、自己是毕业的第一责任人</li>
	  		<li><b>具备</b>：诚实友好、有责任心、上进正能量、乐于交流、独立思考、探索精神、辩证思维的品质</li>
	  		</font>
	  	</ul>
	  </font>
	
</p>

</div>


<h2><font face="Arial" id="News">&#128293 News </font></h2>
<iframe id="News" src="news.html", width=100%, height=280></iframe>

<h2><font face="Arial" id="Publications">&#128218 Selected Works</font> <font size=4> [<a href="https://scholar.google.com/citations?user=ztWK59MAAAAJ" target="_blank">Google Scholar</a>]</font> 
</h2>

<h3 style="margin-top:0px">Brain-Inspired Model: Spiking Neural Networks</h3> 
<table id="tlayout" width="100%">
		<tr>	
    	<td width="240">
				<img src="./images/ICML25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Training High Performance Spiking Neural Network by Temporal Model Calibration </b><br>
				Jiaqi Yan, Changping Wang, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>International Conference on Machine Learning</em> (<i><b>ICML</b></i>), Jul 2025 [<a href="https://icml.cc/virtual/2025/poster/44216">paper</a>][<a href="https://github.com/zju-bmi-lab/TMC">code</a>] (coming soon...)<br>
				<ul>
					<li>Exploit spiking neural networks' temporal heterogeneity through logit gradient analysis</li>
					<li>Highest reported SNN accuracy (up to Jun 2025): <b>85.83%</b> (ImageNet-1K) and <b>87.63±0.15%</b> (CIFAR10-DVS)</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/NIPS24v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor</b><br>
				Mengting Xu, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>Conference on Neural Information Processing Systems</em> (<i><b>NeurIPS</b></i>), Dec 2024 [<a href="https://papers.nips.cc/paper_files/paper/2024/file/a73474c359ed523e6cd3174ed29a4d56-Paper-Conference.pdf">paper</a>][<a href="https://github.com/zju-bmi-lab/FEEL_SNN">code</a>]<br>
				<ul>
					<li>A unified framework for SNN robustness analysis</li>
					<li>An inherentrobust SNN method interoperable with other methods, such as adversarial training </li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/Arxiv24v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting</b><br>
				Weixing Zhang#, Zongrui Li#, De Ma, Huajin Tang, Xudong Jiang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>arXiv preprint arXiv:2410.07266</em> (<i><b>Arxiv</b></i>), Oct 2024 [<a href="https://arxiv.org/abs/2410.07266">paper</a>][<a href="https://github.com/zju-bmi-lab/SpikingGS">code</a>]<br>
				<ul>
					<li>Spiking neuron to reduce excessive Gaussians</li>
					<li>Faster, lower storage, and higher accuracy 3DGS-based surface reconstruction</li>
					<li><b>Stable performance superiority</b> of bio-inspired neural computation over artificial neuron-based computation on core AI tasks</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/AAAI24-liaov1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation <b>(<font color=blue>Oral</font>)</b></b><br>
				Zhanfeng Liao, Yan Liu, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2024 [<a href="https://dl.acm.org/doi/10.1609/aaai.v38i12.29285">paper</a>][<a href="https://github.com/zju-bmi-lab/SpikingNeRF">code</a>][<a href="https://valser.org/article-820-1.html" target="_blank">VALSE<font face="STKaiTi">论文速览</font></a>]<br>
				<ul>
					<li>An ANN-SNN hybrid MLP for NeRF-based 3D surface reconstruction</li>
					<li>Demonstrate the superiority of spiking neuron on 3D surface representation</li>
					<li><b>Stable performance superiority</b> of bio-inspired neural computation over artificial neuron-based computation on core AI tasks</li>
				</ul>
			</td>
		</tr>


			<tr>	
    	<td width="240">
				<img src="./images/TPAMI23-hu.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Fast-SNN: Fast Spiking Neural Network by Converting Quantized ANN </b><br>
				Yangfan Hu#, <b>Qian Zheng</b>#, Xudong Jiang, Gang Pan*.<br>
				<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<i><b>TPAMI</b></i>), Vol. 45, No. 12, Dec 2023 [<a href="https://arxiv.org/abs/2305.19868">paper</a>][<a href="https://github.com/zju-bmi-lab/Fast-SNN">code</a>]<br>
				<ul>
					<li>An ANN2SNN conversion method with high performance and low time step</li>
					<li>Highest reported SNN accuracy (up to Jun 2025): object detection (mAP: <b>73.43%</b>, time steps: 7, PASCAL VOC 2007), and semantic segmentation (mIoU: <b>69.7%</b>, time steps: 15, PASCAL VOC 2012)</li>
				</ul>
			</td>
		</tr>

</table>




<h3 style="margin-top:5px">Brain-Relevant Data: Brain Signals and Event Streams</h3> 
<table id="tlayout" width="100%">

		<tr>	
    	<td width="240">
				<img src="./images/NIPS23v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction (<font color=blue>Spotlight, 378/12343</font>)</b><br>
				Tao Fang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>Conference on Neural Information Processing Systems</em> (<i><b>NeurIPS</b></i>), Dec 2023 [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/3106c718fe84b91fc301fe2f5b738448-Paper-Conference.pdf">paper</a>][<a href="hhttps://github.com/duolala1/GESS">code</a>]<br>
				<ul>
					<li>Alleviate the semantic gap within known and unknown semantic subspaces</li>
					<li>A generalized fMRI-to-image reconstruction method that adaptively weights the semantic and structural information</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/IJCAI25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>EDyGS: Event Enhanced Dynamic 3D Radiance Fields from Blurry Monocular Video</b><br>
				Mengxu Lu#, Zehao Chen#, De Ma*, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>International Joint Conference on Artificial Intelligence</em> (<i><b>IJCAI</b></i>), Aug 2025 [<a href="https://github.com/zju-bmi-lab/EDyGS">paper</a>][<a href="https://github.com/zju-bmi-lab/EDyGS">code</a>] (coming soon...)<br>
				<ul>
					<li>Event-enhanced dynamic 3DGS model taking the input of motion-blurred monocular video</li>
					<li>EReconstruct the motion mask field and separate static and dynamic regions</li>
				</ul>
			</td>
		</tr>



		<tr>	
    	<td width="240">
				<img src="./images/AAAI25-yan.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>EvSTVSR: Event Guided Space-Time Video Super-Resolution</b><br>
				Haojie Yan, Zhan Lu, Zehao Chen, De Ma*, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32983">paper</a>][<a href="https://github.com/hjyyyd/EvSTVSR">code</a>] (coming soon...)<br>
				<ul>
					<li>Event-Guided space-time video super-resolution with fewer adjacent frames</li>
					<li>Excel in handling large motion scenarios</li>
				</ul>
			</td>
		</tr>

<!-- 		<tr>	
	    	<td width="240">
					<img src="./images/AAAI25-chen2.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>EvHDR-GS: Event-guided HDR Video Reconstruction with 3D Gaussian Splatting</b><br>
					Zehao Chen, Zhanfeng Liao, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32237">paper</a>][<a href="https://zehaoc.github.io/EvHDR-GS/">code</a>] (coming soon...)<br>
					<ul>
						<li>Ensure consistent brightness for HDR video reconstruction</li>
						<li>Achieve LDR-to-HDR transformation with single-exposure LDR frames</li>
					</ul>
				</td>
			</tr> -->


		<tr>	
	    	<td width="240">
					<img src="./images/AAAI25-chen1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>EvHDR-NeRF: Building High Dynamic Range Radiance Fields with Single Exposure Images and Events</b><br>
					Zehao Chen, Zhanfeng Liao, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32238">paper</a>][<a href="https://zehaoc.github.io/EvHDR-NeRF/">code</a>] (coming soon...)<br>
					<ul>
						<li>Reconstruct HDR radiance field even if input images are degraded and are single-exposured </li>
						<li>Reconstruct Camera Response Function (CRF) from single-exposure images</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/MM24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Event-ID: Intrinsic Decomposition Using an Event Camera</b><br>
					Zehao Chen#, Zhan Lu#, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>ACM International Conference on Multimedia</em> (<i><b>MM</b></i>), Oct 2024 [<a href="https://dl.acm.org/doi/10.1145/3664647.3681133">paper</a>][<a href="https://zehaoc.github.io/Event-ID//">code</a>] (coming soon...)<br>
					<ul>
						<li>Event-based model establishes relationship between events and intrinsic components </li>
						<li>Multi-view consistency of events to extract specular-related clues</li>
						<li>Event-guided intrinsic decomposition framework enables relighting under extreme conditions</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR21.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Indoor Lighting Estimation using an Event Camera</b><br>
					Zehao Chen#, <b>Qian Zheng</b>#, Peisong Niu, Huajin Tang, Gang Pan*.<br>
					<em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2021 [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Indoor_Lighting_Estimation_Using_an_Event_Camera_CVPR_2021_paper.pdf">paper</a>][<a href="">code</a>] [<a href="">video</a>](coming soon...)<br>
					<ul>
						<li>Event camera captures environmental dynamics during light-activation transients </li>
						<li>Alleviate illumination-distance ambiguity from the inverse-square law in optical imaging</li>
						<li>First attempt to build reflectance model using event streams</li>
					</ul>
				</td>
			</tr>

	</table>




		<h3 style="margin-top:0px">Stereo Vision</h3> 
		<table id="tlayout" width="100%">
			<tr>	
	    	<td width="240">
					<img src="./images/TPAMI25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Revisiting Supervised Learning-Based Photometric Stereo Networks</b><br>
					Xiaoyao Wei, Zongrui Li*, Binjie Ding, Boxin Shi, Xudong Jiang, Gang Pan, Yanlong Cao*, <b>Qian Zheng</b>*<br>
					<em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em> (<i><b>TPAMI</b></i>), Apr 2025 [<a href="https://ieeexplore.ieee.org/document/10948383">paper</a>][<a href="https://github.com/wxy-zju/ESSENCE-Net">code</a>][<a href="https://mp.weixin.qq.com/s/KgD0CBmAIwcAlwEvNGBb2g" target="_blank"><font face="STKaiTi">推文</font></a>]<br>
					<ul>
						<li>Answer three fundamental questions: 1) Waht is the desired deep feature? 2) How to resolve PS challenges? 3) What is the desired network architecture? </li>
						<li>A solution based on answers that achieves SOTA performance on several PS benchmarks</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</b><br>
					Zongrui Li#, Zhan Lu#, Haojie Yan, Boxin Shi, Gang Pan, <b>Qian Zheng</b>*, Xudong Jiang<br>
					<em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2024 [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Spin-UP_Spin_Light_for_Natural_Light_Uncalibrated_Photometric_Stereo_CVPR_2024_paper.pdf">paper</a>][<a href="https://github.com/LMozart/CVPR2024-SpinUP">code</a>]<br>
					<ul>
						<li>A novel setup for Natural Light Uncalibrated Photometric Stereo</li>
						<li>A light prior that leverages object's occluding boundaries for reliable environment light</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR23v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering</b><br>
					Zongrui Li, <b>Qian Zheng</b>*, Boxin Shi, Gang Pan,  Xudong Jiang
					<em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2023 [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DANI-Net_Uncalibrated_Photometric_Stereo_by_Differentiable_Shadow_Handling_Anisotropic_Reflectance_CVPR_2023_paper.pdf">paper</a>][<a href="https://github.com/LMozart/CVPR2023-DANI-Net">code</a>]<br>
					<ul>
						<li>A differentiable shadow handling method and an anisotropic reflectance model</li>
						<li>A self-supervised framework that simultaneously optimizes shape, anisotropic reflectance, shadow map, and light conditions</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/AAAI24-luv1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Pano-NeRF: Synthesizing High Dynamic Range Novel Views with Geometry from Sparse Low Dynamic Range Panoramic Images</b><br>
					Zhan Lu, <b>Qian Zheng</b>*, Boxin Shi, Xudong Jiang
					<em>Proceedings of the Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2024 [<a href="https://arxiv.org/abs/2312.15942">paper</a>][<a href="https://github.com/Lu-Zhan/Pano-NeRF">code</a>]<br>
					<ul>
						<li>An interesting idea of the proposed irradiance fields</li>
						<li>Irradiance fields can be integrated into and jointly optimized with radiance fields to recover the geometry and HDR of a panoramic image</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/ICCV19.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>SPLINE-Net: Sparse Photometric Stereo through Lighting Interpolation and Normal Estimation Networks</b><br>
					<b>Qian Zheng</b>#*, Yiming Jia#, Boxin Shi*, Xudong Jiang, Ling-Yu Duan, Alex C. Kot
					<em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (<i><b>ICCV</b></i>), Oct 2019 [<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zheng_SPLINE-Net_Sparse_Photometric_Stereo_Through_Lighting_Interpolation_and_Normal_Estimation_ICCV_2019_paper.pdf">paper</a>][<a href="https://github.com/yiming-j/SPLINE-Net">code</a>]<br>
					<ul>
						<li>Sparse photometric stereo by generation</li>
						<li>Convert insights of global illumination effects and isotropic reflectance to loss fuction based on <a href="https://arxiv.org/abs/1808.10093">observation map</a> </li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/TIP19.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Numerical Reflectance Compensation for Non-Lambertian Photometric Stereo</b><br>
					<b>Qian Zheng</b>, Ajay Kumar*, Boxin Shi, Gang Pan
					<em> IEEE Transactions on Image Processing </em> (<i><b>TIP</b></i>), Vol 28, Iss 7, Jul 2019) [<a href="https://ieeexplore.ieee.org/document/8625481">paper</a>][<a href="https://github.com/q-zh/numerical-ps">code</a>]<br>
					<ul>
						<li>Reformulate non-Lambertian photometric stereo problem using the angular error to close the gap between evaluation and optimization</li>
						<li>Numerical compensation scheme that solves photometric stereo problem by minizing the angular error</a> </li>
					</ul>	
				</td>
			</tr>


		</table>




	  <h3 style="margin-top:0px">Visual Generation, Reinformance Learning, and Computational Photography </h3> 
		<table id="tlayout" width="100%">

			<tr>	
	    	<td width="240">
					<img src="./images/ECCV24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Connecting Consistency Distillation to Score Distillation for Text-to-3D Generation</b><br>
					Zongrui Li#, Minghui Hu#, <b>Qian Zheng</b>*, Xudong Jiang.<br>
					<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), Sep 2024 [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05982.pdf">paper</a>][<a href="https://github.com/LMozart/ECCV2024-GCS-BEG">code</a>]<br>
					<ul>
						<li>Identify 3 problems in the PF-ODEs-based score distillation method by connecting consistency distillation to score distillation</li>
						<li>An improved score distillation method to enhance details and fidelity of generated 3D assets</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/AAAI24-guo.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Learning to Manipulate Artistic Images</b><br>
					Wei Guo#, Yuqi Zhang#, De Ma*, <b>Qian Zheng</b>*<br>
					<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2024 [<a href="https://arxiv.org/abs/2401.13976">paper</a>][<a href="https://github.com/SnailForce/SIM-Net">code</a>][<a href="https://valser.org/article-823-1.html">VALSE<font face="STKaiTi">论文速览</font></a>]<br>
					<ul>
						<li>An <b>arbitrary</b> Style Image Manipulation Network to achieve zero-shot style image manipulation</li>
						<li>Balance computational efficiency and high resolution</li>
					</ul>	
				</td>
			</tr>


			<tr>	
	    	<td width="240">
					<img src="./images/ICLR25v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>DMitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization</b><br>
					Juntao Dai, Taiye Chen, Yaodong Yang*, <b>Qian Zheng</b>*, Gang Pan<br>
					<em>International Conference on Learning Representations</em> (<i><b>ICLR</b></i>), Apr 2025 [<a href="https://arxiv.org/abs/2503.18130">paper</a>]<br>
					<ul>
						<li>Enable the detection of whether a response is OOD for the reward model.</li>
						<li>The first method that uses value regularization to address reward over-optimization and penalizes OOD values without affecting ID ones</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/ICML24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation</b><br>
					Juntao Dai, Yaodong Yang, <b>Qian Zheng</b>*, Gang Pan*<br>
					<em>International Conference on Machine Learning</em> (<i><b>ICML</b></i>), Jul 2024 [<a href="https://arxiv.org/abs/2412.11138">paper</a>]<br>
					<ul>
						<li>A method to estimate finite-horizon non-discounted constraints in Safe RL tasks</li>
						<li>A deep Safe RL algorithm to address tasks with finite-horizon constraints</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/AAAI23-dai.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Augmented Proximal Policy Optimization for Safe Reinforcement Learning</b><br>
					Juntao Dai#, Jiaming Ji#, Long Yang, <b>Qian Zheng</b>*, Gang Pan*<br>
					<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2023 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25888">paper</a>]<br>
					<ul>
						<li>Construct multiplier-penalty function that dampens cost oscillation for stable convergence while being equivalent to the primal constrained problem to precisely control safety costs</li>
						<li>Provide an implementation based above construction</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR21-zheng.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Single Image Reflection Removal with Absorption Effect</b><br>
					<b>Qian Zheng</b>*, Boxin Shi*, Jinnan Chen, Xudong Jiang, Ling-Yu Duan, Alex C. Kot<br>
					<em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2021 [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zheng_Single_Image_Reflection_Removal_With_Absorption_Effect_CVPR_2021_paper.pdf">paper</a>][<a href="https://github.com/q-zh/absorption">code</a>]<br>
					<ul>
						<li>Reflection-contaminated image formation model considering absorption effect</li>
						<li>Superior performance advantage on the recovery of overall intensity</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR21-hong.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Panoramic Image Reflection Removal</b><br>
					Yuchen Hong#, <b>Qian Zheng</b>#, Lingran Zhao, Xudong Jiang, Alex C. Kot, Boxin Shi*<br>
					<em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2021 [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Panoramic_Image_Reflection_Removal_CVPR_2021_paper.pdf">paper</a>]<br>
					<ul>
						<li>Relieve the content ambiguity for reflection removal using a panoramic image</li>
						<li>Solve the geometric and photometric misalignment between reflection scenes in panoramic and glass-reflected views</li>
						<li>Generalizes well to casual users without panoramic cameras</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR20.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>What does Plate Glass Reveal about Camera Calibration?</b><br>
					<b>Qian Zheng</b>*, Jinnan Chen, Zhan Lu, Boxin Shi*, Xudong Jiang, Kim-Hui Yap, Ling-Yu Duan, and Alex C. Kot<br>
					<em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2020 [<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_What_Does_Plate_Glass_Reveal_About_Camera_Calibration_CVPR_2020_paper.pdf">paper</a>][<a href="https://github.com/q-zh/absorption">code</a>]<br>
					<ul>
						<li>First work addresses the single image camera calibration problem <b>in the context of glass reflection</b></li>
						<li>320 reflection-contaminated images with their calibration parameters</li>
						<li>Alleviate the ill-posedness of single image panoramic image estimation</li>
					</ul>	
				</td>
			</tr>



		</table>




<h2><font face="Arial" id="Activity">&#128187 Activities </font></h2>

<ul>
  <li>Associate Editor
  	<ul>
  		<li style="">IEEE Transactions on Cognitive and Developmental Systems (2024-present)</li>
  		<li>Neurocomputing (2023 - present)</li>
  	</ul>	
	</li>	

  <li>
    Reviewer
    <ul>
      <li>TPAMI, IJCV, TIP, TMM, TNNLS</li>
      <li>CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, SIGGRAPH, AAAI, IJCAI, ACM MM</li>
    </ul>
  </li>
  </li>
  <li>Committee Member
  		<ul>
	      <li>Executive Area Chair Committee for the 7th and 8th Vision And Learning SEminar (<font face="STKaiTi"><a href="https://valser.org/article-796-1.html">第七、八届中国视觉与学习青年学者研讨会执行领域主席副主席委员</a>)</font></li>
	      <li>The Branch of Consciousness Science of the Chinese Cognitive Science Society (<font face="STKaiTi">中国认知科学学会意识科学分会委员</font>)</li>
	      <li>The Branch of Brain-computer Interface & Interaction of the Chinese Neuroscience Society (<font face="STKaiTi">中国神经科学学会脑机分会委员</font>)</li>
    </ul>
  </li>
</ul>

<!-- Young Professionals Committee of the Chinese Association for Artificial Intelligence (中国人工智能协会青年工作委员会委员) -->


<!-- Finance Chair: IEEE CIS-RAM 2024 -->




<h2><font face="Arial" id="Teaching">&#128198 Teaching </font></h2>

<ul>
  <li>Undergraduate Course
  	<ul>
  		<li style="">Theory of Computation (2024-2025 Fall&Winter, Zhejiang University)</li>
  		<li>Theory of Computation (2023-2024 Fall&Winter, Zhejiang University)</li>
  	</ul>	
	</li>	

  <li>
    Graduate Course
    <ul>
      <li>Machine Learning (2023-2024 Summer, Zhejiang University)</li>
      <li>Machine Learning (2022-2023 Summer, Zhejiang University)</li>
    </ul>
  </li>
  </li>
    </ul>
  </li>
</ul>










 
<div id="footer">
	<div id="footer-text"></div>
	
        <p><center>
      	<div id="clustrmaps-widget" style="width:40%">

	<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=wKcWVnadbANTeZkfbjX71Ux442ZlQ7yo13IpNaHQUKg&cl=ffffff&w=a"></script>
 
	



	</div>
	<p><center><font face="Arial">
        <br>
            &copy; Qian Zheng | Last updated: Jun 2025
        </font></center></p>
		
</div>

<!-- <script>
  let iframe = document.getElementById('News');
  let iframeDocument = iframe.contentDocument || iframe.contentWindow.document; //兼容IE

  // 等待iframe内容加载完成
  iframe.onload = function() {
    let links = iframeDocument.querySelectorAll('a');
    for (let i = 0; i < links.length; i++) {
      links[i].style.color = 'red'; // 或者其他颜色值
    }
  };
</script> -->


</body></html>
