<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Qian Zheng, PH, ZJU, Zhejiang University"> 
<meta name="description" content="Qian Zheng's homepage">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Qian Zheng's homepage, Zhejiang University</title>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Georgia"> Qian Zheng </font>   <font face="STKaiTi" size=6 color="0000">  郑   乾</font></h1>
				</div>

				<h3><font face="Georgia" color=000 size=4> <b>Assistant Professor</b> </font></h3>
				<p><font face="Georgia"> 
					College of Computer Science and Technology, <br>
					Zhejiang University, Hangzhou, China. <br>
				
					<em>Email: <a href="qianzheng@zju.edu.cn">qianzheng@zju.edu.cn</a> </em> <br>
					<em>Official Academic Profile: [<a href="https://person.zju.edu.cn/en/zq">En</a>] [<a href="https://person.zju.edu.cn/zq"><font face="STKaiTi">中文</font></a>]</em> <br>
				</font></p>

				<p> <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ztWK59MAAAAJ"><img src="./figs/google_scholariconv1.png" height="80px" style="margin-bottom:-3px"></a>

			  <a href="#News"><img src="./figs/news_iconv1.png" height="80px" style="margin-bottom:-3px"></a>
				
				<a href="#Publications"><img src="./figs/publication_iconv1.png" height="80px" style="margin-bottom:-3px"></a>

				<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ztWK59MAAAAJ"><img src="./figs/activity_iconv1.png" height="80px" style="margin-bottom:-3px"></a>

				<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=ztWK59MAAAAJ"><img src="./figs/teachingv1.png" height="80px" style="margin-bottom:-3px"></a>

				   
				</p>
			</td>

			<td>
				<img src="./figs/profile.jpg" border="0" width="360"><br>
			</td>

		</tr><tr>
	</tr></tbody>
</table>


</head>

<body>
	

<div id="about">
<p style="text-align:justify";><font face="Georgia">
	I'm an Assistant Professor (<font face="STKaiTi">百人计划研究员</font>) at the College of Computer Science and Technology, <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>, and work closely with Prof. <a href="https://person.zju.edu.cn/en/gpan">Gang Pan</a> and Prof. <a href="https://person.zju.edu.cn/en/0018196"> Huajin Tang </a>. I received my B.E. (2011) and Ph.D. (2017, supervisor: Prof. <a href="https://person.zju.edu.cn/en/gpan">Gang Pan</a>) degrees in computer science from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. From 2018 to 2022, I was a Research Fellow at the <a href="https://www.ntu.edu.sg/rose">ROSE Lab</a>, Nanyang Technological University and worked closely with Prof. <a href="https://camera.pku.edu.cn/">Boxin Shi</a>, Prof. <a href="https://personal.ntu.edu.sg/exdjiang/">Xudong Jiang</a>, and Prof. <a href="https://personal.ntu.edu.sg/eackot/">Alex Kot</a>.
</font></p>
<p><font face="Georgia">
	I have co-authored over 60 peer-reviewed papers, with over 10 papers in the IEEE journals and over 30 papers in top conferences of CVPR/ICCV/ECCV/NeurIPS/ICML/ICLR/AAAI. I was a recipient of the ACM Rising Star Award (Hangzhou Chapter, 2024) and currently serves as an Associate Editor for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989"> IEEE Transactions on Cognitive and Developmental Systems </a> and <a href="https://www.sciencedirect.com/journal/neurocomputing">Neurocomputing</a>. 
</font></p>
<p><font face="Georgia">
	My research interests include brain-inspired computing, spiking neural networks, and computer vision. 
</font></p>

</div>


<h2><font face="Arial" id="News">&#128293 News </font></h2>
<iframe id="News" src="news.html", width=100%, height=280></iframe>

<h2><font face="Arial" id="Publications">&#128218 Selected Works</font> <font size=4> [<a href="https://scholar.google.com/citations?user=ztWK59MAAAAJ" target="_blank">Google Scholar</a>]</font> 
</h2>

<h3 style="margin-top:0px">Brain-Inspired Model: Spiking Neural Networks</h3> 
<table id="tlayout" width="100%">
		<tr>	
    	<td width="240">
				<img src="./images/ICML25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Training High Performance Spiking Neural Network by Temporal Model Calibration </b><br>
				Jiaqi Yan, Changping Wang, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>International Conference on Machine Learning</em> (<i><b>ICML</b></i>), Jul 2025 [<a href="https://icml.cc/virtual/2025/poster/44216">paper</a>][<a href="https://github.com/zju-bmi-lab/TMC">code</a>] (coming soon...)<br>
				<ul>
					<li>Exploit spiking neural networks' temporal heterogeneity through logit gradient analysis</li>
					<li>Highest reported SNN accuracy (up to Jun 2025): <b>85.83%</b> (ImageNet-1K) and <b>87.63±0.15%</b> (CIFAR10-DVS)</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/NIPS24v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor</b><br>
				Mengting Xu, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>Conference on Neural Information Processing Systems</em> (<i><b>NeurIPS</b></i>), Dec 2024 [<a href="https://papers.nips.cc/paper_files/paper/2024/file/a73474c359ed523e6cd3174ed29a4d56-Paper-Conference.pdf">paper</a>][<a href="https://github.com/zju-bmi-lab/FEEL_SNN">code</a>]<br>
				<ul>
					<li>A unified framework for SNN robustness analysis</li>
					<li>An inherentrobust SNN method interoperable with other methods, such as adversarial training </li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/Arxiv24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting</b><br>
				Weixing Zhang#, Zongrui Li#, De Ma, Huajin Tang, Xudong Jiang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>arXiv preprint arXiv:2410.07266</em> (<i><b>Arxiv</b></i>), Oct 2024 [<a href="https://arxiv.org/abs/2410.07266">paper</a>][<a href="https://github.com/zju-bmi-lab/SpikingGS">code</a>]<br>
				<ul>
					<li>Spiking neuron to reduce excessive Gaussians</li>
					<li>Faster, lower storage, and higher accuracy 3DGS-based surface reconstruction</li>
					<li><b>Stable performance superiority</b> of bio-inspired neural computation over artificial neuron-based computation on core AI tasks</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/AAAI24-liaov1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation <b>(<font color=blue>Oral</font>)</b></b><br>
				Zhanfeng Liao, Yan Liu, <b>Qian Zheng</b>*, Gang Pan*.<br>
				<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2024 [<a href="https://dl.acm.org/doi/10.1609/aaai.v38i12.29285">paper</a>][<a href="https://github.com/zju-bmi-lab/SpikingNeRF">code</a>][<a href="https://valser.org/article-820-1.html" target="_blank">VALSE<font face="STKaiTi">论文速览</font></a>]<br>
				<ul>
					<li>An ANN-SNN hybrid MLP for NeRF-based 3D surface reconstruction</li>
					<li>Demonstrate the superiority of spiking neuron on 3D surface representation</li>
					<li><b>Stable performance superiority</b> of bio-inspired neural computation over artificial neuron-based computation on core AI tasks</li>
				</ul>
			</td>
		</tr>


			<tr>	
    	<td width="240">
				<img src="./images/TPAMI23-hu.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Fast-SNN: Fast Spiking Neural Network by Converting Quantized ANN </b><br>
				Yangfan Hu#, <b>Qian Zheng</b>#, Xudong Jiang, Gang Pan*.<br>
				<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<i><b>TPAMI</b></i>), Vol. 45, No. 12, Dec 2023 [<a href="https://arxiv.org/abs/2305.19868">paper</a>][<a href="https://github.com/zju-bmi-lab/Fast-SNN">code</a>]<br>
				<ul>
					<li>An ANN2SNN conversion method with high performance and low time step</li>
					<li>Highest reported SNN accuracy (up to Jun 2025): object detection (mAP: <b>73.43%</b>, time steps: 7, PASCAL VOC 2007), and semantic segmentation (mIoU: <b>69.7%</b>, time steps: 15, PASCAL VOC 2012)</li>
				</ul>
			</td>
		</tr>

</table>




<h3 style="margin-top:5px">Brain-Relevant Data: Brain Signals and Event Streams</h3> 
<table id="tlayout" width="100%">

		<tr>	
    	<td width="240">
				<img src="./images/NIPS23v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>Alleviating the Semantic Gap for Generalized fMRI-to-Image Reconstruction (<font color=blue>Spotlight, 378/12343</font>)</b><br>
				Tao Fang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>Conference on Neural Information Processing Systems</em> (<i><b>NeurIPS</b></i>), Dec 2023 [<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/3106c718fe84b91fc301fe2f5b738448-Paper-Conference.pdf">paper</a>][<a href="hhttps://github.com/duolala1/GESS">code</a>]<br>
				<ul>
					<li>Alleviate the semantic gap within known and unknown semantic subspaces</li>
					<li>A generalized fMRI-to-image reconstruction method that adaptively weights the semantic and structural information</li>
				</ul>
			</td>
		</tr>

		<tr>	
    	<td width="240">
				<img src="./images/IJCAI25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>EDyGS: Event Enhanced Dynamic 3D Radiance Fields from Blurry Monocular Video</b><br>
				Mengxu Lu#, Zehao Chen#, De Ma*, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>International Joint Conference on Artificial Intelligence</em> (<i><b>IJCAI</b></i>), Aug 2025 [<a href="https://github.com/zju-bmi-lab/EDyGS">paper</a>][<a href="https://github.com/zju-bmi-lab/EDyGS">code</a>] (coming soon...)<br>
				<ul>
					<li>Event-enhanced dynamic 3DGS model taking the input of motion-blurred monocular video</li>
					<li>EReconstruct the motion mask field and separate static and dynamic regions</li>
				</ul>
			</td>
		</tr>



		<tr>	
    	<td width="240">
				<img src="./images/AAAI25-yan.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
			</td>		
			<td>
				<b>EvSTVSR: Event Guided Space-Time Video Super-Resolution</b><br>
				Haojie Yan, Zhan Lu, Zehao Chen, De Ma*, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan.<br>
				<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32983">paper</a>][<a href="https://github.com/hjyyyd/EvSTVSR">code</a>] (coming soon...)<br>
				<ul>
					<li>Event-Guided space-time video super-resolution with fewer adjacent frames</li>
					<li>Excel in handling large motion scenarios</li>
				</ul>
			</td>
		</tr>

<!-- 		<tr>	
	    	<td width="240">
					<img src="./images/AAAI25-chen2.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>EvHDR-GS: Event-guided HDR Video Reconstruction with 3D Gaussian Splatting</b><br>
					Zehao Chen, Zhanfeng Liao, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32237">paper</a>][<a href="https://zehaoc.github.io/EvHDR-GS/">code</a>] (coming soon...)<br>
					<ul>
						<li>Ensure consistent brightness for HDR video reconstruction</li>
						<li>Achieve LDR-to-HDR transformation with single-exposure LDR frames</li>
					</ul>
				</td>
			</tr> -->


		<tr>	
	    	<td width="240">
					<img src="./images/AAAI25-chen1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>EvHDR-NeRF: Building High Dynamic Range Radiance Fields with Single Exposure Images and Events</b><br>
					Zehao Chen, Zhanfeng Liao, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), Feb 2025 [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32238">paper</a>][<a href="https://zehaoc.github.io/EvHDR-NeRF/">code</a>] (coming soon...)<br>
					<ul>
						<li>Reconstruct the HDR radiance field even when the input images are degraded and with single-exposure images </li>
						<li>Reconstruct Camera Response Function (CRF)</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/MM24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Event-ID: Intrinsic Decomposition Using an Event Camera</b><br>
					Zehao Chen#, Zhan Lu#, De Ma, Huajin Tang, <b>Qian Zheng</b>*, Gang Pan*.<br>
					<em>ACM International Conference on Multimedia</em> (<i><b>MM</b></i>), Oct 2024 [<a href="https://dl.acm.org/doi/10.1145/3664647.3681133">paper</a>][<a href="https://zehaoc.github.io/Event-ID//">code</a>] (coming soon...)<br>
					<ul>
						<li>Event-based model establishes relationship between events and intrinsic components </li>
						<li>Multi-view consistency of events to extract specular-related clues</li>
						<li>Event-guided intrinsic decomposition framework enables relighting under extreme conditions</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR21.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Indoor Lighting Estimation using an Event Camera</b><br>
					Zehao Chen#, <b>Qian Zheng</b>#, Peisong Niu, Huajin Tang, Gang Pan*.<br>
					<em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2021 [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Indoor_Lighting_Estimation_Using_an_Event_Camera_CVPR_2021_paper.pdf">paper</a>][<a href="">code</a>] [<a href="">video</a>](coming soon...)<br>
					<ul>
						<li>Event camera captures environmental dynamics during light-activation transients </li>
						<li>Alleviate illumination-distance ambiguity from the inverse-square law in optical imaging</li>
						<li>First attempt to build reflectance model using event streams</li>
					</ul>
				</td>
			</tr>

	</table>




		<h3 style="margin-top:0px">Stereo Vision</h3> 
		<table id="tlayout" width="100%">
				<tr>	
	    	<td width="240">
					<img src="./images/TPAMI25.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Revisiting Supervised Learning-Based Photometric Stereo Networks</b><br>
					Xiaoyao Wei, Zongrui Li*, Binjie Ding, Boxin Shi, Xudong Jiang, Gang Pan, Yanlong Cao*, <b>Qian Zheng</b>*<br>
					<em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em> (<i><b>TPAMI</b></i>), Apr 2025 [<a href="https://ieeexplore.ieee.org/document/10948383">paper</a>][<a href="https://github.com/wxy-zju/ESSENCE-Net">code</a>]<br>
					<ul>
						<li>Answer three fundamental questions: 1) Waht is the desired deep feature? 2) How to resolve PS challenges 3) What is the desired network architecture </li>
						<li>A solution based on answers that achieves SOTA performance on several PS benchmarks</li>
					</ul>
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</b><br>
					Zongrui Li#, Zhan Lu#, Haojie Yan, Boxin Shi, Gang Pan, <b>Qian Zheng</b>*, Xudong Jiang<br>
					<em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2024 [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Spin-UP_Spin_Light_for_Natural_Light_Uncalibrated_Photometric_Stereo_CVPR_2024_paper.pdf">paper</a>][<a href="https://github.com/LMozart/CVPR2024-SpinUP">code</a>]<br>
					<ul>
						<li>A novel setup for Natural Light Uncalibrated Photometric Stere</li>
						<li>A light prior that leverages object's occluding boundaries for reliable environment light.</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/CVPR23v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>DANI-Net: Uncalibrated Photometric Stereo by Differentiable Shadow Handling, Anisotropic Reflectance Modeling, and Neural Inverse Rendering</b><br>
					Zongrui Li, <b>Qian Zheng</b>*, Boxin Shi, Gang Pan,  Xudong Jiang
					<em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), Jun 2023 [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DANI-Net_Uncalibrated_Photometric_Stereo_by_Differentiable_Shadow_Handling_Anisotropic_Reflectance_CVPR_2023_paper.pdf">paper</a>][<a href="https://github.com/LMozart/CVPR2023-DANI-Net">code</a>]<br>
					<ul>
						<li>A differentiable shadow handling method and an anisotropic reflectance model</li>
						<li>A self-supervised framework that simultaneously optimizes shape, anisotropic reflectance, shadow map, and light conditions</li>
					</ul>	
				</td>
			</tr>




		</table>




	  <h3 style="margin-top:0px">Reinformance Learning, Image Generation, and Computational Photography </h3> 
		<table id="tlayout" width="100%">
			<tr>	
	    	<td width="240">
					<img src="./images/ICLR25v1.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>DMitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization</b><br>
					Juntao Dai, Taiye Chen, Yaodong Yang*, <b>Qian Zheng</b>*, Gang Pan<br>
					<em>International Conference on Learning Representations</em> (<i><b>ICLR</b></i>), Apr 2025 [<a href="https://arxiv.org/abs/2503.18130">paper</a>]<br>
					<ul>
						<li>Enable the detection of whether a response is OOD for the reward model.</li>
						<li>The first method that uses value regularization to address reward over-optimization and penalizes OOD values without affecting ID ones</li>
					</ul>	
				</td>
			</tr>

			<tr>	
	    	<td width="240">
					<img src="./images/ICML24.png" width="210px" height = auto style="box-shadow: 4px 4px 8px #888">
				</td>		
				<td>
					<b>Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation</b><br>
					Juntao Dai, Yaodong Yang, <b>Qian Zheng</b>*, Gang Pan*<br>
					<em>International Conference on Machine Learning</em> (<i><b>ICML</b></i>), Jul 2024 [<a href="https://arxiv.org/abs/2412.11138">paper</a>]<br>
					<ul>
						<li>A method to estimate finite-horizon non-discounted constraints in Safe RL tasks</li>
						<li>A deep Safe RL algorithm to address tasks with finite-horizon constraints</li>
					</ul>	
				</td>
			</tr>


		</table>




<h2><font face="Arial" id="Activity">&#128187 Activities </font></h2>



<h2><font face="Arial" id="Teaching">&#128198 Teaching </font></h2>


 
<div id="footer">
	<div id="footer-text"></div>
	
        <p><center>
      	<div id="clustrmaps-widget" style="width:40%">

	<script type="text/javascript" id="mapmyvisitors" src="//mapmyvisitors.com/map.js?d=wKcWVnadbANTeZkfbjX71Ux442ZlQ7yo13IpNaHQUKg&cl=ffffff&w=a"></script>
 
	



	</div>
	<p><center><font face="Arial">
        <br>
            &copy; Qian Zheng | Last updated: June. 2025
        </font></center></p>
		
</div>

<!-- <script>
  let iframe = document.getElementById('News');
  let iframeDocument = iframe.contentDocument || iframe.contentWindow.document; //兼容IE

  // 等待iframe内容加载完成
  iframe.onload = function() {
    let links = iframeDocument.querySelectorAll('a');
    for (let i = 0; i < links.length; i++) {
      links[i].style.color = 'red'; // 或者其他颜色值
    }
  };
</script> -->


</body></html>
